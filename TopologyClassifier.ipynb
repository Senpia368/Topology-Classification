{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb2e72a-53cd-4d93-86ad-787626cd1298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset.pcd_dataloader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PointCloudDataLoader\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPointCloudProcessor\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\hussa\\OneDrive - University of Tennessee\\UTC\\Research\\TopoClassification\\dataset\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpcd_dataloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PointCloudDataLoader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# ...existing code...\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset.pcd_dataloader'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gudhi\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from dataset import PointCloudDataLoader\n",
    "\n",
    "class PointCloudProcessor:\n",
    "    def __init__(self, normalize=True, num_points=None):\n",
    "        self.normalize = normalize\n",
    "        self.num_points = num_points\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def process(self, point_cloud):\n",
    "        \"\"\"Process a single point cloud.\"\"\"\n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(point_cloud, o3d.geometry.PointCloud):\n",
    "            points = np.asarray(point_cloud.points)\n",
    "        else:\n",
    "            points = point_cloud\n",
    "\n",
    "        # Normalize if requested\n",
    "        if self.normalize:\n",
    "            # Center and scale\n",
    "            centroid = np.mean(points, axis=0)\n",
    "            points = points - centroid\n",
    "            scale = np.max(np.abs(points))\n",
    "            points = points / scale\n",
    "\n",
    "        # Downsample if needed\n",
    "        if self.num_points and len(points) > self.num_points:\n",
    "            indices = np.random.choice(len(points), self.num_points, replace=False)\n",
    "            points = points[indices]\n",
    "\n",
    "        return points\n",
    "\n",
    "class PersistentHomologyFeatures:\n",
    "    def __init__(self, max_dimension=2, max_edge_length=1.0):\n",
    "        self.max_dimension = max_dimension\n",
    "        self.max_edge_length = max_edge_length\n",
    "    \n",
    "    def compute_persistence(self, points):\n",
    "        \"\"\"Compute persistence diagrams for a point cloud.\"\"\"\n",
    "        # Create Vietoris-Rips complex\n",
    "        rips = gudhi.RipsComplex(points=points, max_edge_length=self.max_edge_length)\n",
    "        \n",
    "        # Compute persistence\n",
    "        simplex_tree = rips.create_simplex_tree(max_dimension=self.max_dimension)\n",
    "        persistence = simplex_tree.persistence()\n",
    "        \n",
    "        return persistence\n",
    "    \n",
    "    def extract_features(self, persistence):\n",
    "        \"\"\"Extract features from persistence diagram.\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Process each dimension\n",
    "        for dim in range(self.max_dimension + 1):\n",
    "            # Get persistence pairs for this dimension\n",
    "            pairs = [(birth, death) for (d, (birth, death)) in persistence if d == dim]\n",
    "            \n",
    "            if len(pairs) > 0:\n",
    "                pairs = np.array(pairs)\n",
    "                \n",
    "                # Calculate basic statistics\n",
    "                lifetimes = pairs[:, 1] - pairs[:, 0]\n",
    "                features.extend([\n",
    "                    np.mean(lifetimes),\n",
    "                    np.std(lifetimes),\n",
    "                    np.max(lifetimes),\n",
    "                    len(pairs)  # number of features in this dimension\n",
    "                ])\n",
    "            else:\n",
    "                # Add zeros if no features in this dimension\n",
    "                features.extend([0, 0, 0, 0])\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "class TopologyClassifier:\n",
    "    def __init__(self, processor=None, feature_extractor=None):\n",
    "        self.processor = processor or PointCloudProcessor()\n",
    "        self.feature_extractor = feature_extractor or PersistentHomologyFeatures()\n",
    "        self.classifier = RandomForestClassifier(n_estimators=100)\n",
    "        \n",
    "    def extract_features(self, point_clouds):\n",
    "        \"\"\"Extract features from a list of point clouds.\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for pc in point_clouds:\n",
    "            # Process point cloud\n",
    "            processed_pc = self.processor.process(pc)\n",
    "            \n",
    "            # Compute persistence\n",
    "            persistence = self.feature_extractor.compute_persistence(processed_pc)\n",
    "            \n",
    "            # Extract features from persistence\n",
    "            pc_features = self.feature_extractor.extract_features(persistence)\n",
    "            features.append(pc_features)\n",
    "            \n",
    "        return np.array(features)\n",
    "    \n",
    "    def fit(self, point_clouds, labels):\n",
    "        \"\"\"Train the classifier.\"\"\"\n",
    "        # Extract features\n",
    "        X = self.extract_features(point_clouds)\n",
    "        \n",
    "        # Train classifier\n",
    "        self.classifier.fit(X, labels)\n",
    "    \n",
    "    def predict(self, point_clouds):\n",
    "        \"\"\"Predict labels for point clouds.\"\"\"\n",
    "        # Extract features\n",
    "        X = self.extract_features(point_clouds)\n",
    "        \n",
    "        # Predict\n",
    "        return self.classifier.predict(X)\n",
    "\n",
    "# Deep learning extension\n",
    "class TopologyNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(TopologyNet, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class PointCloudDataset(Dataset):\n",
    "    def __init__(self, point_clouds, labels, processor=None, feature_extractor=None):\n",
    "        self.processor = processor or PointCloudProcessor()\n",
    "        self.feature_extractor = feature_extractor or PersistentHomologyFeatures()\n",
    "        \n",
    "        # Pre-compute features\n",
    "        self.features = []\n",
    "        for pc in point_clouds:\n",
    "            processed_pc = self.processor.process(pc)\n",
    "            persistence = self.feature_extractor.compute_persistence(processed_pc)\n",
    "            features = self.feature_extractor.extract_features(persistence)\n",
    "            self.features.append(features)\n",
    "            \n",
    "        self.features = torch.FloatTensor(self.features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]        \n",
    "\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Generate some dummy data\n",
    "    # num_samples = 100\n",
    "    # num_points = 1000\n",
    "    # num_classes = 4\n",
    "    \n",
    "    # Generate random point clouds (replace with real data)\n",
    "    # point_clouds = [\n",
    "    #     np.random.randn(np.random.randint(500, 1000), 3)\n",
    "    #     for _ in range(num_samples)\n",
    "    # ]\n",
    "    # labels = np.random.randint(0, num_classes, num_samples)\n",
    "    \n",
    "    # # Split data\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #     point_clouds, labels, test_size=0.2, random_state=42\n",
    "    # )\n",
    "    \n",
    "    # # Train classical classifier\n",
    "    # # classifier = TopologyClassifier()\n",
    "    # classifier = TopologyClassifier(processor=PointCloudProcessor(num_points=200))\n",
    "\n",
    "    # print(\"Training classifier...\")\n",
    "    # classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # # Predict\n",
    "    # print(\"Predicting...\")\n",
    "    # predictions = classifier.predict(X_test)\n",
    "    # y_hat = np.array(predictions)\n",
    "    # accuracy = accuracy_score(y_test, y_hat)\n",
    "    # print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    \n",
    "    # # Train deep learning model\n",
    "    # # Create datasets\n",
    "    # train_dataset = PointCloudDataset(X_train, y_train)\n",
    "    # test_dataset = PointCloudDataset(X_test, y_test)\n",
    "    \n",
    "    # # Create data loaders\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    # test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # # Initialize model\n",
    "    # input_dim = train_dataset.features.shape[1]\n",
    "    # model = TopologyNet(input_dim=input_dim, num_classes=num_classes)\n",
    "    \n",
    "    # # Training loop (simplified)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # num_epochs = 10\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     model.train()\n",
    "    #     epoch_loss = 0\n",
    "    #     for features, labels in train_loader:\n",
    "    #         optimizer.zero_grad()\n",
    "    #         outputs = model(features)\n",
    "    #         loss = criterion(outputs, labels)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         epoch_loss += loss.item()\n",
    "    #     print(f\"Epoch {epoch+1}, loss: {epoch_loss / len(train_loader)}\")\n",
    "    dataset_path = 'merged_sampled_objects'\n",
    "    data = PointCloudDataLoader(dataset_path, use_cache=False).data\n",
    "    print(type(data))\n",
    "    print(data.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48f2e2",
   "metadata": {},
   "source": [
    "# Sample enough objects from cropped classes\n",
    "# 1500 for each class\n",
    "# Test the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed4d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
